{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yT3BB_3lZJuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c460e2-ab8f-4804-c31c-a79f41c5e943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 105])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_height=64, img_width=128, patch_size=8, overlap=4, in_chans=1, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.overlap = overlap\n",
        "        stride = patch_size - overlap\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class GrnnNet(nn.Module):\n",
        "    def __init__(self, in_chans=1, num_classes=105, img_height=64, img_width=128, patch_size=8, overlap=4, embed_dim=256, depth=4, num_heads=4, mlp_ratio=2., mode='vertical'):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.patch_embed = PatchEmbedding(img_height, img_width, patch_size, overlap, in_chans, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, (img_height - overlap) // (patch_size - overlap) * (img_width - overlap) // (patch_size - overlap) + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=0.1)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim * mlp_ratio))\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        if self.mode == 'vertical':\n",
        "            x = x[:, :x.size(1) // 2].mean(dim=1)\n",
        "        elif self.mode == 'horizontal':\n",
        "            x = x[:, x.size(1) // 2:].mean(dim=1)\n",
        "        else:\n",
        "            x = x.mean(dim=1)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.rand(1, 1, 64, 128)\n",
        "    mod = GrnnNet(in_chans=1, num_classes=105, mode='vertical')\n",
        "    logits = mod(x)\n",
        "    print(logits.shape)\n"
      ]
    }
  ]
}